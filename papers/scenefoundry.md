# SCENEFOUNDRY: Generating Interactive Infinite 3D Worlds

URL: https://anc891203.github.io/SceneFoundry-Demo/#

## Summary
SceneFoundry is a framework designed to generate large-scale, interactive 3D apartment environments for robotic training and embodied AI. It starts by using an LLM to guide a procedural generation engine, creating structurally valid floor plans from natural language prompts. To populate these layouts, it employs a diffusion model that generates furniture arrangements in parallel, while enforcing functional logic through specific constraints that prevent articulated object collisions (like blocked drawers) and guarantee walkable paths for navigation.

# Notes

## 1. Why Diffusion Model?
The paper explicitly chooses diffusion models over other methods (like autoregressive models) for three main technical reasons:

- **Global Coherence & Parallel Generation**: Unlike autoregressive models (e.g., ATISS) that place objects one by one and suffer from "error accumulation" (if the first object is wrong, the whole room breaks), diffusion models generate all object parameters in parallel. This ensures the room layout "makes sense" holistically.
- **Flexible Editing**: Diffusion allows for "holistic editing". You can modify parts of the scene without breaking the rest.
- **Constraint Guidance (Posterior Sampling)**: This is the biggest selling point. They use "diffusion posterior sampling" which allows them to apply differentiable constraints (like "don't collide" or "put exactly 5 chairs") at test time without retraining the whole model.

## 2. Use Cases & Adjustability

**Use Cases:**
- Paper: Robotic Learning and Embodied AI (training robots to navigate houses without needing a physical house).
- What I think: Game/Vtuber Potential
- Enterprise Use: For companies like IKEA or interior design firms, this could automate initial layout concepts. For tech companies (like NVIDIA/Tesla), this generates "synthetic data" to train their AI agents.

**Adjustability (Room/Object Plan):**
- Position Adjustment: Yes. The "Walkable Area Control" optimizes the scene after generation to ensure agents can walk through it, modifying positions to fix navigation paths.
- Size/Dimensions: Yes. The post-processing algorithm explicitly "modifies only object sizes while preserving their placements" to meet spatial requirements.
- Collision Control: They use a specific "Articulated Collision Constraint" to ensure functional parts (like drawers opening) don't hit other objects.

## 3. Output Formats & Enterprise Readiness

File Formats: The pipeline is extremely enterprise-friendly and supports standard industry formats. Figure 1 explicitly lists the export formats:

- USD (Universal Scene Description - standard for Pixar/NVIDIA Omniverse)
- GLTF / GLB (Standard for web 3D and simple game engines)
- .Blend (Blender source files)

Enterprise Integration:

The paper mentions integration with NVIDIA Omniverse, Genesis, and MuJoCo. These are heavy-hitter simulation platforms used by major tech companies for robotics and "digital twins."

# Other Highlights & Tech Stack
(Generated by Gemini)
- The "Renderer" of Choice: They used **Blender Cycles** for all visualizations. This confirms that Python scripting in Blender (bpy) is a critical skill for this type of research.
- Hardware Note: They actually used an NVIDIA RTX 5090 for rendering! (This is quite new/rare in papers).
- Datasets: They combined 3D-FRONT (for layouts) with GAPartNet (for interactive/articulated objects like opening doors).
- LLM Integration: They use an LLM (Large Language Model) to convert natural language (e.g., "I want a 3-bedroom house") into the mathematical parameters required for the procedural generation. This "Text-to-Param-to-3D" workflow is a very modern pattern in AI engineering.